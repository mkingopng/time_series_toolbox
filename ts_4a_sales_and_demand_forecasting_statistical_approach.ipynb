{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "similarly to volatility clustering, intermittent time series is a problem usually (but not exclusively) encountered in business applications. When predicting sales or demand in retail, we have three main issues to deal with: data is (frequently) integer-valued, intermittent and not that large. We will discuss those different aspects in the following sections:\n",
    "\n",
    "* [Croston model](#section-one)\n",
    "* [ML approach](#section-two)\n",
    "* [New launches](#section-three)"
   ],
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.030438,
     "end_time": "2022-03-06T18:06:24.272153",
     "exception": false,
     "start_time": "2022-03-06T18:06:24.241715",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from data_sources import *"
   ],
   "metadata": {
    "papermill": {
     "duration": 1.280622,
     "end_time": "2022-03-06T18:06:25.592438",
     "exception": false,
     "start_time": "2022-03-06T18:06:24.311816",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2022-03-25T21:43:51.794056Z",
     "iopub.execute_input": "2022-03-25T21:43:51.794482Z",
     "iopub.status.idle": "2022-03-25T21:43:51.805125Z",
     "shell.execute_reply.started": "2022-03-25T21:43:51.794437Z",
     "shell.execute_reply": "2022-03-25T21:43:51.803945Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "warnings.simplefilter(action='ignore', category= FutureWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# general settings\n",
    "class CFG:\n",
    "    data_folder = data\n",
    "    img_dim1 = 20\n",
    "    img_dim2 = 10\n",
    "        \n",
    "# adjust the parameters for displayed figures    \n",
    "plt.rcParams.update({'figure.figsize': (CFG.img_dim1,CFG.img_dim2)})    "
   ],
   "metadata": {
    "papermill": {
     "duration": 0.032606,
     "end_time": "2022-03-06T18:06:25.651664",
     "exception": false,
     "start_time": "2022-03-06T18:06:25.619058",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2022-03-25T21:15:04.500835Z",
     "iopub.execute_input": "2022-03-25T21:15:04.501231Z",
     "iopub.status.idle": "2022-03-25T21:15:04.509087Z",
     "shell.execute_reply.started": "2022-03-25T21:15:04.501183Z",
     "shell.execute_reply": "2022-03-25T21:15:04.507884Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def my_rmse(x,y):\n",
    "    \"\"\"\n",
    "\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return np.round(np.sqrt(mse(x, y)), 4)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-25T21:15:04.514387Z",
     "iopub.execute_input": "2022-03-25T21:15:04.5151Z",
     "iopub.status.idle": "2022-03-25T21:15:04.524332Z",
     "shell.execute_reply.started": "2022-03-25T21:15:04.515056Z",
     "shell.execute_reply": "2022-03-25T21:15:04.523434Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will be using the data from the M5 competition: https://www.kaggle.com/c/m5-forecasting-accuracy. The contest used a hierarchical sales data from Walmart to forecast daily sales for the next 28 days. From the comp description page: \"*The data covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events*\". \n",
    "\n",
    "\n",
    "We start by formatting the data to a more usable format."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "xdat = pd.read_csv(sales_train_validation)\n",
    "xdat = xdat.loc[xdat.state_id == 'CA']\n",
    "xdat.head(5)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/m5_forecast_accuracy/sales_train_validation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m xdat \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43msales_train_validation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m xdat \u001B[38;5;241m=\u001B[39m xdat\u001B[38;5;241m.\u001B[39mloc[xdat\u001B[38;5;241m.\u001B[39mstate_id \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCA\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      3\u001B[0m xdat\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m5\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/time_series/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001B[0m, in \u001B[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m>\u001B[39m num_allow_args:\n\u001B[1;32m    306\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    307\u001B[0m         msg\u001B[38;5;241m.\u001B[39mformat(arguments\u001B[38;5;241m=\u001B[39marguments),\n\u001B[1;32m    308\u001B[0m         \u001B[38;5;167;01mFutureWarning\u001B[39;00m,\n\u001B[1;32m    309\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39mstacklevel,\n\u001B[1;32m    310\u001B[0m     )\n\u001B[0;32m--> 311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/time_series/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    665\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    666\u001B[0m     dialect,\n\u001B[1;32m    667\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    676\u001B[0m     defaults\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelimiter\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m,\u001B[39m\u001B[38;5;124m\"\u001B[39m},\n\u001B[1;32m    677\u001B[0m )\n\u001B[1;32m    678\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 680\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/time_series/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    572\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    574\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 575\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m~/anaconda3/envs/time_series/lib/python3.9/site-packages/pandas/io/parsers/readers.py:934\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    931\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    933\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 934\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/time_series/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1218\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1214\u001B[0m     mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1215\u001B[0m \u001B[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001B[39;00m\n\u001B[1;32m   1216\u001B[0m \u001B[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001B[39;00m\n\u001B[1;32m   1217\u001B[0m \u001B[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001B[39;00m\n\u001B[0;32m-> 1218\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[call-overload]\u001B[39;49;00m\n\u001B[1;32m   1219\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1220\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1221\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1222\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1223\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1224\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1225\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1226\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1227\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1228\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1229\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m~/anaconda3/envs/time_series/lib/python3.9/site-packages/pandas/io/common.py:786\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    785\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 786\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    787\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    793\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    794\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    795\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/m5_forecast_accuracy/sales_train_validation.csv'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\"\n",
    "    iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "\n",
    "xdat = reduce_mem_usage(xdat)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# melt the data into long format\n",
    "xdat = pd.melt(xdat, id_vars=[\n",
    "    'id',\n",
    "    'item_id',\n",
    "    'dept_id',\n",
    "    'cat_id',\n",
    "    'store_id',\n",
    "    'state_id'\n",
    "], var_name='d', value_name='sales').dropna()\n",
    "\n",
    "# get proper timestamps\n",
    "xcal = pd.read_csv(calendar)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "xdat = pd.merge(\n",
    "    xdat,\n",
    "    xcal,\n",
    "    on='d',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "del xcal"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "xdat.drop([\n",
    "    'd',\n",
    "    'wm_yr_wk',\n",
    "    'weekday',\n",
    "    'month',\n",
    "    'year',\n",
    "    'event_name_1',\n",
    "    'event_type_1',\n",
    "    'event_name_2',\n",
    "    'event_type_2',\n",
    "    'snap_TX',\n",
    "    'snap_WI',\n",
    "    'state_id'\n",
    "], axis=1, inplace = True)\n",
    "\n",
    "xdat.head(3)"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "That looks more manageable - let's get started.\n",
    "\n",
    "\n",
    "<a id=\"section-one\"></a>\n",
    "# Croston model\n",
    "\n",
    "## Basic Croston\n",
    "\n",
    "A first simple approach to predicting intermittent demand series is the Croston model, which takes a three step approach:\n",
    "- evaluate the average demand level when there is a demand occurrence\n",
    "- evaluate the average time between two demand occurrences\n",
    "- forecast the demand as the demand level (when there is an occurrence) multiplied by the probability of having an occurrence.\n",
    "\n",
    "We go about this by adjusting exponential smoothing to our problem. If we denote the actual demand level by $X_t$ and our level estimate as $a_t$, then if $X_t > 0$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "a_{t+1} = \\alpha X_t + (1- \\alpha) a_t \n",
    "\\end{equation}\n",
    "\n",
    "and $a_{t+1} = a_t$ otherwise; $\\alpha$ has the same role as in the basic exponential smoothing (see https://www.kaggle.com/konradb/ts-1a-smoothing-methods for a refresher). \n",
    "\n",
    "A second important component of a model for an intermittent time series is periodicity: we capture time between two demand occurrences $p$ and time elapsed time elapsed since the previous demand occurrence $q$. As before, for $X_t > 0$ we can specify\n",
    "\n",
    "\\begin{equation}\n",
    "p_{t+1} = \\alpha q + (1-\\alpha) p_t\n",
    "\\end{equation}\n",
    "\n",
    "and $p_{t+1} = p_t$ otherwise. The forecast for a given period is the given by\n",
    "\n",
    "\\begin{equation}\n",
    "f_{t+1} = \\frac{a_t}{p_t}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "How does it perform in practice? Let's begin by subsetting our dataset to a single product x store combination:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df = xdat.loc[(xdat.item_id == 'HOBBIES_1_288') & (xdat.store_id == 'CA_1') ][['date', 'sales']].copy()\n",
    "\n",
    "df = df.loc[(df.date >= '2012-01-01') & (df.date <= '2015-06-30')]\n",
    "\n",
    "df.set_index('date').sales.plot()"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Dislacimer: the code for both the basic Croston model and the TSB variant (discussed below) is taken from the excellent post by Nicolas Vandeput:\n",
    "\n",
    "https://medium.com/towards-data-science/croston-forecast-model-for-intermittent-demand-360287a17f5f"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def croston(ts, extra_periods=1, alpha=0.4):\n",
    "    \"\"\"\n",
    "\n",
    "    :param ts:\n",
    "    :param extra_periods:\n",
    "    :param alpha:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    d = np.array(ts)  # Transform the input into a numpy array\n",
    "    cols = len(d)  # Historical period length\n",
    "    d = np.append(d, [np.nan] * extra_periods)  # Append np.nan into the demand array to cover future periods\n",
    "    \n",
    "    # level (a), periodicity(p) and forecast (f)\n",
    "    a, p, f = np.full((3, cols + extra_periods), np.nan)\n",
    "    q = 1 #periods since last demand observation\n",
    "    \n",
    "    # Initialization\n",
    "    first_occurrence = np.argmax(d[:cols]>0)\n",
    "    a[0] = d[first_occurrence]\n",
    "    p[0] = 1 + first_occurrence\n",
    "    f[0] = a[0] / p[0]\n",
    "    # Create all the t+1 forecasts\n",
    "    for t in range(0, cols):\n",
    "        if d[t] > 0:\n",
    "            a[t + 1] = alpha*d[t] + (1 - alpha) * a[t]\n",
    "            p[t + 1] = alpha*q + (1 - alpha) * p[t]\n",
    "            f[t + 1] = a[t + 1] / p[t + 1]\n",
    "            q = 1           \n",
    "        else:\n",
    "            a[t + 1] = a[t]\n",
    "            p[t + 1] = p[t]\n",
    "            f[t + 1] = f[t]\n",
    "            q += 1\n",
    "       \n",
    "    # Future Forecast \n",
    "    a[cols + 1: cols + extra_periods] = a[cols]\n",
    "    p[cols + 1: cols + extra_periods] = p[cols]\n",
    "    f[cols + 1: cols + extra_periods] = f[cols]\n",
    "    df = pd.DataFrame.from_dict({\n",
    "        \"Demand\": d,\n",
    "        \"Forecast\": f,\n",
    "        \"Period\": p,\n",
    "        \"Level\": a,\n",
    "        \"Error\": d-f\n",
    "    })\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generate a forecast with base Croston method:"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pred_croston =  croston(df.sales, extra_periods=10)\n",
    "pred_croston"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pred_croston = pred_croston.Forecast.head(len(df))\n",
    "print('RMSE: ' + str(my_rmse(df.sales, pred_croston)))"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# xvalid = pd.DataFrame(xvalid.values, columns = ['actual'])\n",
    "df['Croston'] = pred_croston.values\n",
    "df.set_index('date').plot()"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "If there is a period with no demand, the forecast from the Croston model is not updated - which is counterintuitive:\n",
    "- extended period with no demand should lead to a reduced forecast going forward\n",
    "- surge in demand after a long period of inactivity should lead to an increase\n",
    "\n",
    "In 2011 Teunter, Synteos and Babai proposed an extension to the basic Croston model: https://www.sciencedirect.com/science/article/abs/pii/S0377221711004437. The main change they proposed was allowing the model to decrease the periodicity estimate, even in the absence of demand. \n",
    "\n",
    "The level of demand is estimated in the same manner as before\n",
    "\n",
    "\\begin{equation}\n",
    "a_{t+1} = \\alpha X_t + (1- \\alpha) a_t \n",
    "\\end{equation}\n",
    "\n",
    "We change the definition of periodicity: $p$ denotes the probability of having a demand occurrence and it will be updated each period:\n",
    "- decrease if there is no demand occurrence, in an exponential manner \n",
    "- increase otherwise\n",
    "\n",
    "\\begin{equation}\n",
    "p_{t+1} = \\beta + (1- \\beta) p_t\n",
    "\\end{equation}\n",
    "\n",
    "if $X_t > 0$ and $(1- \\beta) p_t$ otherwise. The forecast for a given period is the given by\n",
    "\n",
    "\\begin{equation}\n",
    "f_{t+1} = a_{t+1}p_{t+1}\n",
    "\\end{equation}\n",
    "\n",
    "Caveat emptor:\n",
    "- the forecast f is defined as the periodicity p multiplied by the level a (**and not divided by it, as in the original model**)\n",
    "- the forecast for t+1 is defined based on the level and periodicity estimates of t+1 (**and not t**)\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def croston_tsb(ts, extra_periods=1, alpha=0.4, beta=0.1):\n",
    "    \"\"\"\n",
    "\n",
    "    :param ts:\n",
    "    :param extra_periods:\n",
    "    :param alpha:\n",
    "    :param beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    d = np.array(ts)  # Transform the input into a numpy array\n",
    "    cols = len(d)  # Historical period length\n",
    "    d = np.append(d, [np.nan] * extra_periods)  # Append np.nan into the demand array to cover future periods\n",
    "\n",
    "    a, p, f = np.full((3, cols + extra_periods), np.nan)  # level (a), probability(p) and forecast (f)\n",
    "    first_occurrence = np.argmax(d[:cols] > 0)  # Initialization\n",
    "    a[0] = d[first_occurrence]\n",
    "    p[0] = 1 / (1 + first_occurrence)\n",
    "    f[0] = p[0] * a[0]\n",
    "                 \n",
    "    # Create all the t+1 forecasts\n",
    "    for t in range(0, cols):\n",
    "        if d[t] > 0:\n",
    "            a[t + 1] = alpha * d[t] + (1 - alpha) * a[t]\n",
    "            p[t + 1] = beta * 1 + (1 - beta) * p[t]\n",
    "        else:\n",
    "            a[t + 1] = a[t]\n",
    "            p[t + 1] = (1 - beta) * p[t]\n",
    "        f[t + 1] = p[t + 1] * a[t + 1]\n",
    "    # Future Forecast\n",
    "    a[cols + 1: cols + extra_periods] = a[cols]\n",
    "    p[cols + 1: cols + extra_periods] = p[cols]\n",
    "    f[cols + 1: cols + extra_periods] = f[cols]\n",
    "    df = pd.DataFrame.from_dict({\n",
    "        \"Demand\": d,\n",
    "        \"Forecast\": f,\n",
    "        \"Period\": p,\n",
    "        \"Level\": a,\n",
    "        \"Error\": d-f\n",
    "    })\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pred_croston_tsb = croston_tsb(\n",
    "    df.sales,\n",
    "    extra_periods=10,\n",
    "    alpha=0.1,\n",
    "    beta=0.1\n",
    ")\n",
    "\n",
    "pred_croston_tsb = pred_croston_tsb.Forecast.head(len(df))\n",
    "\n",
    "print('RMSE: ' + str(my_rmse(df.sales,pred_croston_tsb)))"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df['Croston_TSB'] = pred_croston_tsb.values\n",
    "df.set_index('date').plot()"
   ],
   "metadata": {
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Takeaway:\n",
    "Moving from vanilla to TSB Croston:\n",
    "- improves the error (somewhat)\n",
    "- solves the issues with forecast adjustment in zero sales periods\n",
    "- time series as regression\n",
    "- (lagged) rolling statistics\n",
    "- distribution of target\n",
    "- careful about ranges $\\rightarrow$ extrapolation\n",
    "\n",
    "Overall, it gives us a way of handling an intermittent time series, but **it does not have a natural extension to a multivariate case** - so we need to try something else in that instance."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}